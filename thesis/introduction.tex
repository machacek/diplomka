\chapter{Introduction}

The field of machine translation (MT) experienced a very fast development over
the past twenty years. It was primarily caused by the growing power of
computers which allowed researchers to start using statistical methods which
require a lot of computer resources to both learn statistics from data and then
to use them in translation.

In the globalized world we live in, there is a need for translating from one
language to another. The translation itself is often not easy even for people
who have to be trained for that and therefore well paid. There is therefore a
very high demand for cheap and high-quality machine translation. A lot of
researchers and companies try to satisfy this demand and constantly improve
their translations system. The first presumption for improving your system is
that you know how to measure the improvement.

There are actually more situation in which you need to evaluate machine
translation. If you are a customer who would like to buy a machine translation
system you want to buy the system which will best suit you. There are many
criteria of MT systems (speed, price, memory consumption, scalability etc.) but
the most important criterion is usually the machine translation quality.
Therefore, you would like to evaluate this quality on a sample of documents you
are going to translate.

If you are a developer of a machine translation system you need to evaluate
your system during different phases of the development process. Let us go
through the development phases which need an evaluation in reversed order. From
time to time, you have a ready-to-sell or ready-to-use system and you would like
to have a comparison to your competitors

competitors


For all these reasons, we explore the area of evaluating machine translation
quality in this thesis.

Unlike other applications of machine learning, the evaluation in machine
translation is not easy at all. The main reason for that is that when
translating an average sentence there is no single correct solution, in fact
there are hundreds of thousands correct translations
\parcite{bojar2013scratching}. Unlike other problems




\begin{itemize}
  \item zminit prekotny vyvoj strojoveho prekladu
  \item proc je potreba merit kvalitu
  \item zakladni pristupy mereni kvality, manualni, automaticke, dalsi zpusoby 
\end{itemize}




\begin{comment}
Machine Translation quality can be measured in two different ways: using human
evaluation or automatic metrics.  Altough human evaluation is considered more
accurate than automatic evaluation, it suffers some disadvantages.  It is slow
and expensive and therefore cannot be used in tuning parameters of statistical
models.

The aim of this thesis is to develop a new semi-automatic evaluation measure,
which would have advantages of both human evaluation and automatic evaluation.
The idea is to evaluate small sentence segments by human and create a database
of such annotation which could be used later to automatically evaluate new
unseen sentences.  This new measure should evaluate MT outputs more similarly
to how human do and still be cheap and fast after the initial database of
annotations is created once. It could be therefore used in tuning parameters of
MT systems.

The important part of the thesis is to design and develop a new annotation
environment which will be used to collect the annotations. This will be then used
to annotate wmt14 test data. The new measure will be analysed and compared to
current manual and automatic evaluation measures. 
\end{comment}

\section{Motivation and Goals}

\section{Outline}

\XXX{\parcite{wmt14-overview-paper}}
\XXX{\parcite{bojar2012cestina}}




