\chapter{Introduction}

\begin{comment}
The field of machine translation (MT) experienced a very fast development over
the past twenty years. It was primarily caused by the growing power of
computers allowing researchers to start using statistical methods. These methods
require a lot of computer resources to both learn statistics from data and then
to use them in translation.
\end{comment}

In the globalized world we live in, there is a need for translating from one
human language to another. The translation itself is often not easy even for
people. They have to be trained for that and therefore well paid. There is
therefore a very high demand for cheap and high-quality machine translation. A
lot of researchers and companies try to satisfy this demand and constantly
improve their translation system. One of the most important thing when improving
your system is that you know how to measure the improvement.

There are a lot of situations in which you need to evaluate machine
translation. If you are a customer who would like to buy a machine translation
system you want to buy the system which will best suit you. There are many
criteria of MT systems (speed, price, memory consumption, scalability, etc.) but
the most important criterion is usually the machine translation quality.
Therefore, you would like to evaluate this quality on a sample of documents you
are going to translate.

If you are a developer of a machine translation system you need to evaluate
your system during various phases of the development process. Let us go through
the phases which need an evaluation in reversed order. From time to time, you
have a ready-to-use system and you would like to have a comparison to systems
of your competitors or another researchers. You can evaluate your system
yourself in a way that the obtained score is comparable to other scores, or you
can participate in a shared evaluation campaign, for instance NIST
OpenMT\footnote{\url{http://www.nist.gov/itl/iad/mig/openmt12.cfm}} or at
Workshop on Statistical Machine
Translation\footnote{\url{http://www.statmt.org/wmt14/translation-task.html}}
(WMT) organized by \perscite{wmt14-overview-paper}.  During the day-to-day
development process of your system, you try various configuration, new
components and features. You have to evaluate your system with every change to
know how much the change improved (or sometimes worsen) the translation
quality. Recently, new automatic methods which tune parameters of your system
to directly optimize an evaluation measure emerged.

Unlike other applications of natural language processing, for instance speech
recognition, the evaluation in machine translation is not easy at all. The main
reason for that is that when translating an average sentence, there is no single
correct solution, in fact there are hundreds of thousands correct translations
\parcite{bojar2013scratching}.

There are two fundamental approaches in the machine translation evaluation: a
manual evaluation and an automatic evaluation \parcite{bojar2012cestina}. In
the manual evaluation, human judges assess the quality of each sentence
manually. The most widely used methodology in the past was to assign values
from two five point scales representing fluency and adequacy. However, it was
shown that a) people had difficulties with separating these two aspects of
translation and b) there was a very small inter-annotator agreement, since each
annotator had different expectation of a good translation. In later evaluation
campaigns started by \perscite{wmt-overview-2007}, annotators ranked
translations relatively to each other.

In the automatic evaluation, an output of a machine translation system is
compared to a reference translation. However, unlike other machine learning
problems, you cannot simply compute the proportion of translated sentences
which match the reference translation. If a translated sentence is not
identical to its reference counterpart it does not have to be a bad translation
because there is the very large number of correct translations. Automatic
metrics\footnote{Automatic metrics for machine translation evaluation are not
actually metrics according to the mathematical definition. For example, the
triangle inequality usually does not hold and some metrics are not even
symmetric. However, we are going to use this traditional term in this thesis.}
are used to measure the similarity between the candidate and reference
translations. The more similar they are, the better the translation is
considered.

As you can see, the evaluation of machine translation is very important and at
the same time, very difficult. We explore both manual and automatic evaluation
in this thesis.

\section{Motivation and Goals}

Manual evaluation is of course considered as the only source of truth which
metrics try to approximate. However, it suffers from many disadvantages. Since
it includes manual labour, it is very costly and slow. Moreover, manual
evaluation is not reproducible; human judges have different criteria for
comparing candidates and even an individual judge is not consistent with
himself in time. Human evaluation is therefore most often used in shared
evaluation campaigns and sometimes used when you want to evaluate a new component
of your system. It is definitely not feasible to directly use human evaluation
in an automatic method for tuning your model's parameters because these methods
require to evaluate millions of sentences.

Automatic metrics, on the other hand, are fast, reproducible and cost almost
nothing. However, they are only proxies to human evaluation and their
correlation to human judgments varies a lot. The first goal of this thesis is
therefore to conduct a large-scale comparison of available automatic metrics in
the terms of correlation with human judgments. For this purpose, we have
organized Metrics Shared Task within Workshop on Statistical Machine
Translation in two consecutive years 2013 \parcite{machacek:2013} and 2014
\parcite{machacek:2014}. We also compare the metrics in other more subjective
criteria. 

It would be very useful if we have an evaluation method which would take
advantages from both manual and automatic evaluations. Recently, there actually
emerged such methods on the boundary of manual and automatic evaluation. These
methods usually require a large manual annotation effort at the beginning to
create a database and then they use the collected database in an automatic way
during evaluation.  The second goal of this thesis is to propose a method which
could be used to manually evaluate a set of systems and the database collected
during this manual evaluation could be later reused to automatically evaluate
new, unseen systems and to tune a system. This goal includes, besides proposing
the method, also developing an annotation application, conducting a real
evaluation experiment and experiment with reusing the collected database. 

\section{Outline}

The thesis is organized as follows. In Chapter \ref{chapter:segranks} we
describe the manual evaluation method and report the annotation experiment we
have conducted. We explore the possibility of reusing the collected database to
evaluate new systems and to tune a system in Chapter \ref{chapter:experiments}.
In this chapter, we also analyze the results in more details and try to explain
them.  The automatic metrics are described, compared and evaluated in terms
of correlation with human judgments in Chapter \ref{metrics}. Related work is
summarized in Chapter \ref{chapter:related}.  We conclude our work in Chapter
\ref{chapter:conclusion}.


There are three appendices, all of them relate to the contents of the attached
CD-ROM.  In Appendix \ref{metrics-documentation}, you can find the user
documentation of the metrics task package which is used to compute the metrics
task results. In Appendix \ref{segranks-documentation}, you can find the user
documentation of the annotation application called \textit{SegRanks} which is
used in the annotation experiment. You can find the development documentation 
of this application in Appendix \ref{chapter:implementation}.






\begin{comment}
Machine Translation quality can be measured in two different ways: using human
evaluation or automatic metrics.  Altough human evaluation is considered more
accurate than automatic evaluation, it suffers some disadvantages.  It is slow
and expensive and therefore cannot be used in tuning parameters of statistical
models.

The aim of this thesis is to develop a new semi-automatic evaluation measure,
which would have advantages of both human evaluation and automatic evaluation.
The idea is to evaluate small sentence segments by human and create a database
of such annotation which could be used later to automatically evaluate new
unseen sentences.  This new measure should evaluate MT outputs more similarly
to how human do and still be cheap and fast after the initial database of
annotations is created once. It could be therefore used in tuning parameters of
MT systems.

The important part of the thesis is to design and develop a new annotation
environment which will be used to collect the annotations. This will be then used
to annotate wmt14 test data. The new measure will be analysed and compared to
current manual and automatic evaluation measures. 
\end{comment}





