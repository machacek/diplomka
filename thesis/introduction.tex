\chapter{Introduction}

Citing \cite{wmt14-overview-paper}.

Machine Translation quality can be measured in two different ways: using human
evaluation or automatic metrics.  Altough human evaluation is considered more
accurate than automatic evaluation, it suffers some disadvantages.  It is slow
and expensive and therefore cannot be used in tuning parameters of statistical
models.

The aim of this thesis is to develop a new semi-automatic evaluation measure,
which would have advantages of both human evaluation and automatic evaluation.
The idea is to evaluate small sentence segments by human and create a database
of such annotation which could be used later to automatically evaluate new
unseen sentences.  This new measure should evaluate MT outputs more similarly
to how human do and still be cheap and fast after the initial database of
annotations is created once. It could be therefore used in tuning parameters of
MT systems.

The important part of the thesis is to design and develop a new annotation
environment which will be used to collect the annotations. This will be then used
to annotate wmt14 test data. The new measure will be analysed and compared to
current manual and automatic evaluation measures. 

\section{Human Evaluation}
\XXX{Strucny uvod do lidske evaluace}

\section{Automatic Metrics}
\XXX{Strucny uvod do automatickych metrik}

\section{Motivation for Semi-Automatic Evaluation}
\XXX{Zde shrnu vyhody a nevyhody lidske a automaticke evaluace. Co a jak by semi-automaticka evaluace resila}

\section{Division of the Thesis}






