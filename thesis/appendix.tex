\appendix
\chapter{WMT14 Metrics Task Package Documentation}

\noindent
The \texttt{wmt14-metrics-task} directory located in the attached CD-ROM
contains scripts and data behind the results of WMT14 Metrics Task. The
makefile in this directory is used to generate all the results. The command
\texttt{make all} creates the following files which contain the results: 

\begin{itemize}
  \item \texttt{system.correlations.toEn}
  \item \texttt{system.correlations.fromEn}
  \item \texttt{segment.correlations.toEn}
  \item \texttt{segment.correlations.fromEn}
\end{itemize}

\noindent
If you want to reproduce also the confidence intervals, change the following
variables in Makefile:

\begin{verbatim}
    COMPUTE_CONFIDENCE = true
    SEGMENT_BOOTSTRAP_SAMPLES = 1000
\end{verbatim}

\noindent
You can use this package to evaluate your metric(s) on wmt14 data and compare
it with other metrics. Create a subdirectory in \texttt{submissions/} and put
there your metrics data files in the submission format as described at
\url{http://www.statmt.org/wmt14/metrics-task/}.  The file names have to end
with \texttt{*.sys.score} or \texttt{*.seg.score} file extensions.

\vspace{0.7cm}
\noindent
The scripts in this package have the following requirements:
 
\begin{itemize}
  \item The \texttt{baselines/Makefile} requires a path to a compiled Moses, set
    the \texttt{MOSESROOT} env. variable.
  \item Scripts require Python 3 with \textit{scipy} and \textit{tabulate} packages installed.
\end{itemize}

\noindent Important content of this package:
\begin{itemize}
  \item \texttt{metrics-task-paper.pdf} - the published paper with WMT14 metrics task results
  \item \texttt{submissions/} - the metrics data submitted by the task participants
  \item \texttt{baselines/} - the computation of the baseline metrics
  \item \texttt{compute-segment-correlations} - see \texttt{./compute-segment-correlations --help}
  \item \texttt{compute-system-correlations} - see \texttt{./compute-segment-correlations --help}
  \item \texttt{judgements-2014-05-14.csv} - the raw human judgements
  \item \texttt{human-2014-05-16.scores} - the official system-level human scores (TrueSkill)
  \item \texttt{human-2014-05-16.folded/} - the human scores computed on generated folds
\end{itemize}


\chapter{SegRanks Application Documentation}

The \texttt{segranks} directory located in the attached CD-ROM contains
\textit{SegRanks}, the annotation application which is used for ranking short
segments of machine translation.

\section{Installing and Running the Application}

\textit{SegRanks} is a Django web application written in Python and requires
Python 2.7 and some additional Python dependencies.  They are listed in
\texttt{requirements.txt} and you can easily install them to a new virtual
environment using the following command:

\begin{verbatim}
$ virtualenv /path/to/new/environment
$ . /path/to/new/environment/bin/activate
$ pip install -r requirements.txt
\end{verbatim}

\noindent
If you want to run the application locally, an \textit{sqlite3} database is
used. To initialize it and apply all database migrations, run:

\begin{verbatim}
$ ./manage.py syncdb 
$ ./manage.py migrate 
\end{verbatim}

\noindent
The database is now initialized and you can start the local web server

\begin{verbatim}
$ ./manage.py runserver
\end{verbatim}

\noindent
and open the application in your browser (usually \url{http://127.0.0.1:8000/}).
However, the list of annotation project will be empty. You need to create
a project and upload extracted segments. This is described in Section \ref{creating-project}.

The application is also ready to be deployed to Heroku cloud. To do that, you
need in essence to create a Heroku application, initialize a Git repository
with \textit{SegRanks}, add Heroku as a remote and push your commits there.
It will automatically install all the dependencies. The application is
configured to automatically set the database when deployed to Heroku. Please
see Heroku documentation for more details.

\section{Creating a New Annotation Project}
\label{creating-project}

To create a new annotation project, you need to have a file with extracted
segments.  The file has one row for each segment with the following tab
separated fields:

\begin{enumerate}
  \item sentence ID
  \item tokenized source sentence
  \item tokenized reference sentence translation
  \item tokenized source segment
  \item tokenized candidate segment 
  \item zero based indices of source segment words separated by a space (used for highlighting the segment in source sentence)
\end{enumerate}

\noindent
You can use the attached file \texttt{extracted.segments}, which contains
segments extracted for the experiment in my thesis. To create a project, run:

\begin{verbatim}
$ ./manage.py create_project extracted.segments \
                "<Project Name>" "<Project Description>"
\end{verbatim}

\section{Annotating}

Annotating in the application is very easy. Before you start, you need to be
registered and signed in. To start annotating, select an annotation project you
want to work on. You will be then shown annotation instructions and an
annotated sentence.  For each annotated segment, drag and drop segment
candidates into the ranks.  When all the segment candidates are placed in the
ranks, the submit button is enabled and you can submit your annotation to
server. A new sentence to annotate will be displayed. 

\section{Printing Annotation Statistics}

You can list annotators with various statistics (number of annotated segments,
time spent annotating, agreements, etc.) with the following command:

\begin{verbatim}
$ ./manage.py statistics <project_id>
\end{verbatim}

\noindent
To get list of available projects with their IDs, run the command without the
argument.

\section{Exporting the Database}

To export segments with their ranks in JSON or in Python Pickle format, use one
of the following commands:

\begin{verbatim}
$ ./manage.py export_project <project_id> <out_file> json
$ ./manage.py export_project <project_id> <out_file> pickle
\end{verbatim}

\noindent Both formats store a dictionary indexed by tuples of sentence IDs and
source segments. The values of this dictionary are list of rank dictionaries.
The rank dictionary is indexed by candidate segments and its values are
assigned ranks. The following listing is an example JSON output:

\begin{verbatim}
{
    "2386,Writing books saved me .": [
        {
            "Knihy psaní uložily mě .": 5,
            "Napsané knihy ušetřily mě .": 4,
            "Psaní knih mě zachránil .": 1,
            "Psaní knihy mě zachránil .": 2,
            "Psaní knihy mě zachránily .": 2,
            "Písemné knihy mě zachránily .": 5
        }
    ],
    "2755,At each station": [
        {
            "Na každé stanici": 1,
            "U každé stanice": 2,
            "V každé stanici": 2
        }
    ],
}
\end{verbatim}

\noindent
You can also find all the annotated segments from experiments in my thesis in
the file \texttt{annotated.segments}.
